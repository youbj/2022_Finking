{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import FinanceDataReader as fdr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "# from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph() \n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화\n",
    "def data_standardization(x):\n",
    "    x_np = np.asarray(x)\n",
    "    return (x_np - x_np.mean()) / x_np.std()\n",
    " \n",
    "# 너무 작거나 너무 큰 값이 학습을 방해하는 것을 방지하고자 정규화한다\n",
    "# x가 양수라는 가정하에 최소값과 최대값을 이용하여 0~1사이의 값으로 변환\n",
    "# Min-Max scaling\n",
    "def min_max_scaling(x):\n",
    "    x_np = np.asarray(x)\n",
    "    return (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-7) # 1e-7은 0으로 나누는 오류 예방차원\n",
    " \n",
    "# 정규화된 값을 원래의 값으로 되돌린다\n",
    "# 정규화하기 이전의 org_x값과 되돌리고 싶은 x를 입력하면 역정규화된 값을 리턴한다\n",
    "def reverse_min_max_scaling(org_x, x):\n",
    "    org_x_np = np.asarray(org_x)\n",
    "    x_np = np.asarray(x)\n",
    "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_column_cnt = 3  # 입력데이터의 컬럼 개수(Variable 개수)\n",
    "output_data_column_cnt = 1 # 결과데이터의 컬럼 개수\n",
    "\n",
    "seq_length = 30      # 1개 시퀀스의 길이\n",
    "rnn_cell_hidden_dim = 20   # 각 셀의 (hidden)출력 크기\n",
    "forget_bias = 1.0          # 망각편향\n",
    "num_stacked_layers = 1     # stacked LSTM layers 개수\n",
    "keep_prob = 0.2            # dropout할 때 keep할 비율\n",
    "\n",
    "epoch_num = 1000           # 에폭 횟수(학습에서 훈련 데이터를 모두 소진했을 때의 횟수)\n",
    "learning_rate = 0.01       # 학습률 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-01-02</th>\n",
       "      <td>6160</td>\n",
       "      <td>5660</td>\n",
       "      <td>6160</td>\n",
       "      <td>5620</td>\n",
       "      <td>1142079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-03</th>\n",
       "      <td>6040</td>\n",
       "      <td>6100</td>\n",
       "      <td>6190</td>\n",
       "      <td>6020</td>\n",
       "      <td>885636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-04</th>\n",
       "      <td>6380</td>\n",
       "      <td>6380</td>\n",
       "      <td>6410</td>\n",
       "      <td>6290</td>\n",
       "      <td>1009482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-07</th>\n",
       "      <td>6360</td>\n",
       "      <td>6260</td>\n",
       "      <td>6430</td>\n",
       "      <td>6170</td>\n",
       "      <td>1029132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-08</th>\n",
       "      <td>6230</td>\n",
       "      <td>6390</td>\n",
       "      <td>6390</td>\n",
       "      <td>6190</td>\n",
       "      <td>760142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>62600</td>\n",
       "      <td>63100</td>\n",
       "      <td>63200</td>\n",
       "      <td>62300</td>\n",
       "      <td>16631445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>60400</td>\n",
       "      <td>62500</td>\n",
       "      <td>62500</td>\n",
       "      <td>60400</td>\n",
       "      <td>15331184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>60300</td>\n",
       "      <td>60900</td>\n",
       "      <td>61100</td>\n",
       "      <td>60000</td>\n",
       "      <td>13767787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>59200</td>\n",
       "      <td>59800</td>\n",
       "      <td>60100</td>\n",
       "      <td>59200</td>\n",
       "      <td>13888300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>58900</td>\n",
       "      <td>58800</td>\n",
       "      <td>59600</td>\n",
       "      <td>58500</td>\n",
       "      <td>11984658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5175 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close   Open   High    Low    Volume\n",
       "Date                                            \n",
       "2002-01-02   6160   5660   6160   5620   1142079\n",
       "2002-01-03   6040   6100   6190   6020    885636\n",
       "2002-01-04   6380   6380   6410   6290   1009482\n",
       "2002-01-07   6360   6260   6430   6170   1029132\n",
       "2002-01-08   6230   6390   6390   6190    760142\n",
       "...           ...    ...    ...    ...       ...\n",
       "2022-12-01  62600  63100  63200  62300  16631445\n",
       "2022-12-02  60400  62500  62500  60400  15331184\n",
       "2022-12-05  60300  60900  61100  60000  13767787\n",
       "2022-12-06  59200  59800  60100  59200  13888300\n",
       "2022-12-07  58900  58800  59600  58500  11984658\n",
       "\n",
       "[5175 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fdr.DataReader('005930','2002-01-01','')\n",
    "all_data = data[['Close','Open','High','Low','Volume']].round(2)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_info = data.values[:].astype(np.float) # 금액&거래량 문자열을 부동소수점형으로 변환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = stock_info[:,:-2] # 시작가, 최고가, 최저가, 종료가\n",
    "price.shape\n",
    "len(stock_info)\n",
    "for i in range(len(stock_info)):\n",
    "    if price[i][1]==0:\n",
    "        price[i][1]=price[i][3]\n",
    "\n",
    "for i in range(len(stock_info)):\n",
    "    if price[i][2]==0:\n",
    "        price[i][2]=price[i][3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_price = min_max_scaling(price) # 가격형태 데이터 정규화 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volume.shape:  (5175, 1)\n",
      "volume[0]:  [1142079.]\n",
      "norm_volume[0]:  [0.01264674]\n"
     ]
    }
   ],
   "source": [
    "# 거래량형태 데이터를 정규화한다\n",
    "# ['Open','High','Low','Close','Adj Close','Volume']에서 마지막 'Volume'만 취함\n",
    "# [:,-1]이 아닌 [:,-1:]이므로 주의하자! 스칼라가아닌 벡터값 산출해야만 쉽게 병합 가능\n",
    "volume = stock_info[:,-2:-1]\n",
    "norm_volume = min_max_scaling(volume) # 거래량형태 데이터 정규화 처리\n",
    "print(\"volume.shape: \", volume.shape)\n",
    "print(\"volume[0]: \", volume[0])\n",
    "print(\"norm_volume[0]: \", norm_volume[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (5175, 5)\n",
      "x[0]:  [0.05847107 0.06363636 0.05805785 0.06363636 0.01264674]\n",
      "x[-1]:  [0.60743802 0.61570248 0.60433884 0.60847107 0.13271139]\n"
     ]
    }
   ],
   "source": [
    "# 행은 그대로 두고 열을 우측에 붙여 합친다\n",
    "x = np.concatenate((norm_price, norm_volume), axis=1) # axis=1, 세로로 합친다\n",
    "print(\"x.shape: \", x.shape)\n",
    "print(\"x[0]: \", x[0])    # x의 첫 값\n",
    "print(\"x[-1]: \", x[-1])  # x의 마지막 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y[0]:  [0.06363636]\n",
      "y[-1]:  [0.60847107]\n"
     ]
    }
   ],
   "source": [
    "y = x[:, [-2]] # 타켓은 주식 종가이다\n",
    "print(\"y[0]: \",y[0])     # y의 첫 값\n",
    "print(\"y[-1]: \",y[-1])   # y의 마지막 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = [] # 입력으로 사용될 Sequence Data\n",
    "dataY = [] # 출력(타켓)으로 사용\n",
    "\n",
    "for i in range(0, len(y) - seq_length):\n",
    "    _x = x[i : i+seq_length]\n",
    "    _y = y[i + seq_length] # 다음 나타날 주가(정답)\n",
    "    dataX.append(_x) # dataX 리스트에 추가\n",
    "    dataY.append(_y) # dataY 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ts_train_test(data, time_steps, for_periods):\n",
    "#     ts_train= data['2002-01-01':'2016-10-14'].iloc[:,0:1].values\n",
    "#     ts_test= data['2016-10-14':].iloc[:,0:1].values\n",
    "#     ts_train_len = len(ts_train)\n",
    "#     ts_test_len = len(ts_test)\n",
    "    \n",
    "#     #정규화\n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "#     sc= MinMaxScaler(feature_range=(0,1))\n",
    "#     ts_train_scaled = sc.fit_transform(ts_train)\n",
    "    \n",
    "#     #training data의 samples와 time steps로 원본데이터 슬라이싱하기\n",
    "#     X_train=[]\n",
    "#     Y_train=[]\n",
    "#     Y_train_stacked=[]\n",
    "#     for i in range(time_steps,ts_train_len-1):\n",
    "#         X_train.append(ts_train[i-time_steps:i,0])\n",
    "#         Y_train.append(ts_train[i:i+for_periods,0])\n",
    "#     X_train,Y_train=np.array(X_train), np.array(Y_train)\n",
    "    \n",
    "#     X_train = np.reshape(X_train,(X_train.shape[0], X_train.shape[1],1))\n",
    "    \n",
    "#     inputs = pd.concat((data['Close']['2002-01-01':'2016-10-14'],data['Close']['2016-10-14':]), axis=0).values\n",
    "#     inputs = inputs[len(inputs)-len(ts_test)-time_steps:]\n",
    "#     inputs = inputs.reshape(-1,1)\n",
    "    \n",
    "#     X_test = []\n",
    "#     for i in range(time_steps, ts_test_len+ time_steps-for_periods):\n",
    "#         X_test.append(inputs[i-time_steps:i,0])\n",
    "#     X_test = np.array(X_test)\n",
    "#     X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))\n",
    "    \n",
    "#     return X_train, Y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5660</td>\n",
       "      <td>6100</td>\n",
       "      <td>6380</td>\n",
       "      <td>6260</td>\n",
       "      <td>6390</td>\n",
       "      <td>6210</td>\n",
       "      <td>6360</td>\n",
       "      <td>6240</td>\n",
       "      <td>6050</td>\n",
       "      <td>6430</td>\n",
       "      <td>...</td>\n",
       "      <td>7219</td>\n",
       "      <td>7370</td>\n",
       "      <td>7100</td>\n",
       "      <td>7000</td>\n",
       "      <td>6880</td>\n",
       "      <td>6910</td>\n",
       "      <td>6890</td>\n",
       "      <td>6680</td>\n",
       "      <td>6699</td>\n",
       "      <td>[6660, 7019, 6800]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6100</td>\n",
       "      <td>6380</td>\n",
       "      <td>6260</td>\n",
       "      <td>6390</td>\n",
       "      <td>6210</td>\n",
       "      <td>6360</td>\n",
       "      <td>6240</td>\n",
       "      <td>6050</td>\n",
       "      <td>6430</td>\n",
       "      <td>6150</td>\n",
       "      <td>...</td>\n",
       "      <td>7370</td>\n",
       "      <td>7100</td>\n",
       "      <td>7000</td>\n",
       "      <td>6880</td>\n",
       "      <td>6910</td>\n",
       "      <td>6890</td>\n",
       "      <td>6680</td>\n",
       "      <td>6699</td>\n",
       "      <td>6660</td>\n",
       "      <td>[7019, 6800, 6900]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6380</td>\n",
       "      <td>6260</td>\n",
       "      <td>6390</td>\n",
       "      <td>6210</td>\n",
       "      <td>6360</td>\n",
       "      <td>6240</td>\n",
       "      <td>6050</td>\n",
       "      <td>6430</td>\n",
       "      <td>6150</td>\n",
       "      <td>5900</td>\n",
       "      <td>...</td>\n",
       "      <td>7100</td>\n",
       "      <td>7000</td>\n",
       "      <td>6880</td>\n",
       "      <td>6910</td>\n",
       "      <td>6890</td>\n",
       "      <td>6680</td>\n",
       "      <td>6699</td>\n",
       "      <td>6660</td>\n",
       "      <td>7019</td>\n",
       "      <td>[6800, 6900, 7059]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6260</td>\n",
       "      <td>6390</td>\n",
       "      <td>6210</td>\n",
       "      <td>6360</td>\n",
       "      <td>6240</td>\n",
       "      <td>6050</td>\n",
       "      <td>6430</td>\n",
       "      <td>6150</td>\n",
       "      <td>5900</td>\n",
       "      <td>6010</td>\n",
       "      <td>...</td>\n",
       "      <td>7000</td>\n",
       "      <td>6880</td>\n",
       "      <td>6910</td>\n",
       "      <td>6890</td>\n",
       "      <td>6680</td>\n",
       "      <td>6699</td>\n",
       "      <td>6660</td>\n",
       "      <td>7019</td>\n",
       "      <td>6800</td>\n",
       "      <td>[6900, 7059, 6770]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6390</td>\n",
       "      <td>6210</td>\n",
       "      <td>6360</td>\n",
       "      <td>6240</td>\n",
       "      <td>6050</td>\n",
       "      <td>6430</td>\n",
       "      <td>6150</td>\n",
       "      <td>5900</td>\n",
       "      <td>6010</td>\n",
       "      <td>5800</td>\n",
       "      <td>...</td>\n",
       "      <td>6880</td>\n",
       "      <td>6910</td>\n",
       "      <td>6890</td>\n",
       "      <td>6680</td>\n",
       "      <td>6699</td>\n",
       "      <td>6660</td>\n",
       "      <td>7019</td>\n",
       "      <td>6800</td>\n",
       "      <td>6900</td>\n",
       "      <td>[7059, 6770, 7000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>30120</td>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "      <td>30240</td>\n",
       "      <td>30660</td>\n",
       "      <td>30400</td>\n",
       "      <td>31380</td>\n",
       "      <td>31360</td>\n",
       "      <td>30960</td>\n",
       "      <td>30380</td>\n",
       "      <td>...</td>\n",
       "      <td>31640</td>\n",
       "      <td>31420</td>\n",
       "      <td>31000</td>\n",
       "      <td>31080</td>\n",
       "      <td>31459</td>\n",
       "      <td>31800</td>\n",
       "      <td>32200</td>\n",
       "      <td>32020</td>\n",
       "      <td>33920</td>\n",
       "      <td>[34000, 33000, 31999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "      <td>30240</td>\n",
       "      <td>30660</td>\n",
       "      <td>30400</td>\n",
       "      <td>31380</td>\n",
       "      <td>31360</td>\n",
       "      <td>30960</td>\n",
       "      <td>30380</td>\n",
       "      <td>30580</td>\n",
       "      <td>...</td>\n",
       "      <td>31420</td>\n",
       "      <td>31000</td>\n",
       "      <td>31080</td>\n",
       "      <td>31459</td>\n",
       "      <td>31800</td>\n",
       "      <td>32200</td>\n",
       "      <td>32020</td>\n",
       "      <td>33920</td>\n",
       "      <td>34000</td>\n",
       "      <td>[33000, 31999, 29900]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>30000</td>\n",
       "      <td>30240</td>\n",
       "      <td>30660</td>\n",
       "      <td>30400</td>\n",
       "      <td>31380</td>\n",
       "      <td>31360</td>\n",
       "      <td>30960</td>\n",
       "      <td>30380</td>\n",
       "      <td>30580</td>\n",
       "      <td>31320</td>\n",
       "      <td>...</td>\n",
       "      <td>31000</td>\n",
       "      <td>31080</td>\n",
       "      <td>31459</td>\n",
       "      <td>31800</td>\n",
       "      <td>32200</td>\n",
       "      <td>32020</td>\n",
       "      <td>33920</td>\n",
       "      <td>34000</td>\n",
       "      <td>33000</td>\n",
       "      <td>[31999, 29900, 31000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>30240</td>\n",
       "      <td>30660</td>\n",
       "      <td>30400</td>\n",
       "      <td>31380</td>\n",
       "      <td>31360</td>\n",
       "      <td>30960</td>\n",
       "      <td>30380</td>\n",
       "      <td>30580</td>\n",
       "      <td>31320</td>\n",
       "      <td>31480</td>\n",
       "      <td>...</td>\n",
       "      <td>31080</td>\n",
       "      <td>31459</td>\n",
       "      <td>31800</td>\n",
       "      <td>32200</td>\n",
       "      <td>32020</td>\n",
       "      <td>33920</td>\n",
       "      <td>34000</td>\n",
       "      <td>33000</td>\n",
       "      <td>31999</td>\n",
       "      <td>[29900, 31000, 30960]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>30660</td>\n",
       "      <td>30400</td>\n",
       "      <td>31380</td>\n",
       "      <td>31360</td>\n",
       "      <td>30960</td>\n",
       "      <td>30380</td>\n",
       "      <td>30580</td>\n",
       "      <td>31320</td>\n",
       "      <td>31480</td>\n",
       "      <td>31340</td>\n",
       "      <td>...</td>\n",
       "      <td>31459</td>\n",
       "      <td>31800</td>\n",
       "      <td>32200</td>\n",
       "      <td>32020</td>\n",
       "      <td>33920</td>\n",
       "      <td>34000</td>\n",
       "      <td>33000</td>\n",
       "      <td>31999</td>\n",
       "      <td>29900</td>\n",
       "      <td>[31000, 30960]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3611 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9   \\\n",
       "0      5660   6100   6380   6260   6390   6210   6360   6240   6050   6430   \n",
       "1      6100   6380   6260   6390   6210   6360   6240   6050   6430   6150   \n",
       "2      6380   6260   6390   6210   6360   6240   6050   6430   6150   5900   \n",
       "3      6260   6390   6210   6360   6240   6050   6430   6150   5900   6010   \n",
       "4      6390   6210   6360   6240   6050   6430   6150   5900   6010   5800   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "3606  30120  30000  30000  30240  30660  30400  31380  31360  30960  30380   \n",
       "3607  30000  30000  30240  30660  30400  31380  31360  30960  30380  30580   \n",
       "3608  30000  30240  30660  30400  31380  31360  30960  30380  30580  31320   \n",
       "3609  30240  30660  30400  31380  31360  30960  30380  30580  31320  31480   \n",
       "3610  30660  30400  31380  31360  30960  30380  30580  31320  31480  31340   \n",
       "\n",
       "      ...     41     42     43     44     45     46     47     48     49  \\\n",
       "0     ...   7219   7370   7100   7000   6880   6910   6890   6680   6699   \n",
       "1     ...   7370   7100   7000   6880   6910   6890   6680   6699   6660   \n",
       "2     ...   7100   7000   6880   6910   6890   6680   6699   6660   7019   \n",
       "3     ...   7000   6880   6910   6890   6680   6699   6660   7019   6800   \n",
       "4     ...   6880   6910   6890   6680   6699   6660   7019   6800   6900   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "3606  ...  31640  31420  31000  31080  31459  31800  32200  32020  33920   \n",
       "3607  ...  31420  31000  31080  31459  31800  32200  32020  33920  34000   \n",
       "3608  ...  31000  31080  31459  31800  32200  32020  33920  34000  33000   \n",
       "3609  ...  31080  31459  31800  32200  32020  33920  34000  33000  31999   \n",
       "3610  ...  31459  31800  32200  32020  33920  34000  33000  31999  29900   \n",
       "\n",
       "                         0   \n",
       "0        [6660, 7019, 6800]  \n",
       "1        [7019, 6800, 6900]  \n",
       "2        [6800, 6900, 7059]  \n",
       "3        [6900, 7059, 6770]  \n",
       "4        [7059, 6770, 7000]  \n",
       "...                     ...  \n",
       "3606  [34000, 33000, 31999]  \n",
       "3607  [33000, 31999, 29900]  \n",
       "3608  [31999, 29900, 31000]  \n",
       "3609  [29900, 31000, 30960]  \n",
       "3610         [31000, 30960]  \n",
       "\n",
       "[3611 rows x 51 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, Y_train, X_test = ts_train_test(data,seq_length,3)\n",
    "\n",
    "# X_train_see= pd.DataFrame(np.reshape(X_train, (X_train.shape[0], X_train.shape[1])))\n",
    "# Y_train_see= pd.DataFrame(Y_train)\n",
    "# pd.concat([X_train_see, Y_train_see], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_normalize(all_data,time_steps,for_periods):\n",
    "\n",
    "    # create training and test set\n",
    "    ts_train= data['2002-01-01':'2016-10-14'].iloc[:,0:1].values\n",
    "    ts_test= data['2016-10-14':].iloc[:,0:1].values\n",
    "    ts_train_len = len(ts_train)\n",
    "    ts_test_len = len(ts_test)\n",
    "\n",
    "    # scale the data\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    sc = MinMaxScaler(feature_range=(0,1))\n",
    "    ts_train_scaled = sc.fit_transform(ts_train)\n",
    "\n",
    "    # create training data of s samples and t time steps\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    y_train_stacked = []\n",
    "    for i in range(time_steps,ts_train_len-1): \n",
    "        X_train.append(ts_train_scaled[i-time_steps:i,0])\n",
    "        y_train.append(ts_train_scaled[i:i+for_periods,0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "    # Reshaping X_train for efficient modelling\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "\n",
    "    inputs = pd.concat((all_data[\"Close\"]['2002-01-01':'2016-10-14'], all_data[\"Close\"]['2016-10-14':]),axis=0).values\n",
    "    inputs = inputs[len(inputs)-len(ts_test) - time_steps:]\n",
    "    inputs = inputs.reshape(-1,1)\n",
    "    inputs  = sc.transform(inputs)\n",
    "\n",
    "    # Preparing X_test\n",
    "    X_test = []\n",
    "    for i in range(time_steps,ts_test_len+time_steps-for_periods):\n",
    "        X_test.append(inputs[i-time_steps:i,0])\n",
    "        \n",
    "    X_test = np.array(X_test)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "    return X_train, y_train , X_test, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train= data['2002-01-01':'2016-10-14'].iloc[:,0:1].values\n",
    "ts_test= data['2016-10-14':].iloc[:,0:1].values\n",
    "ts_train_len = len(ts_train)\n",
    "ts_test_len = len(ts_test)\n",
    "for a in range(ts_test_len):\n",
    "    if ts_test[a]==0:\n",
    "        ts_test[a]=ts_test[a-1]\n",
    "        \n",
    "for a in range(ts_train_len):\n",
    "    if ts_train[a]==0:\n",
    "        ts_train[a]=ts_train[a-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011855</td>\n",
       "      <td>0.027197</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>0.032775</td>\n",
       "      <td>0.037308</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.032078</td>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.038703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029289</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.049477</td>\n",
       "      <td>0.056485</td>\n",
       "      <td>[0.052266387726638774, 0.05509065550906553, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027197</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>0.032775</td>\n",
       "      <td>0.037308</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.032078</td>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.038703</td>\n",
       "      <td>0.028940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.049477</td>\n",
       "      <td>0.056485</td>\n",
       "      <td>0.052266</td>\n",
       "      <td>[0.05509065550906553, 0.04811715481171547, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036960</td>\n",
       "      <td>0.032775</td>\n",
       "      <td>0.037308</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.032078</td>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.038703</td>\n",
       "      <td>0.028940</td>\n",
       "      <td>0.020223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.049477</td>\n",
       "      <td>0.056485</td>\n",
       "      <td>0.052266</td>\n",
       "      <td>0.055091</td>\n",
       "      <td>[0.04811715481171547, 0.05369595536959554, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.032775</td>\n",
       "      <td>0.037308</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.032078</td>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.038703</td>\n",
       "      <td>0.028940</td>\n",
       "      <td>0.020223</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.049477</td>\n",
       "      <td>0.056485</td>\n",
       "      <td>0.052266</td>\n",
       "      <td>0.055091</td>\n",
       "      <td>0.048117</td>\n",
       "      <td>[0.05369595536959554, 0.042887029288702916, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037308</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.032078</td>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.038703</td>\n",
       "      <td>0.028940</td>\n",
       "      <td>0.020223</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.049477</td>\n",
       "      <td>0.056485</td>\n",
       "      <td>0.052266</td>\n",
       "      <td>0.055091</td>\n",
       "      <td>0.048117</td>\n",
       "      <td>0.053696</td>\n",
       "      <td>[0.042887029288702916, 0.04497907949790794, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>0.981869</td>\n",
       "      <td>0.975593</td>\n",
       "      <td>0.986053</td>\n",
       "      <td>0.951185</td>\n",
       "      <td>0.934449</td>\n",
       "      <td>0.931660</td>\n",
       "      <td>0.963040</td>\n",
       "      <td>0.958856</td>\n",
       "      <td>0.918410</td>\n",
       "      <td>0.926778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917713</td>\n",
       "      <td>0.910042</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.898187</td>\n",
       "      <td>0.911402</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.937238</td>\n",
       "      <td>0.930962</td>\n",
       "      <td>0.997211</td>\n",
       "      <td>[1.0, 0.9651324965132496, 0.9302301255230127]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3627</th>\n",
       "      <td>0.975593</td>\n",
       "      <td>0.986053</td>\n",
       "      <td>0.951185</td>\n",
       "      <td>0.934449</td>\n",
       "      <td>0.931660</td>\n",
       "      <td>0.963040</td>\n",
       "      <td>0.958856</td>\n",
       "      <td>0.918410</td>\n",
       "      <td>0.926778</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910042</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.898187</td>\n",
       "      <td>0.911402</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.937238</td>\n",
       "      <td>0.930962</td>\n",
       "      <td>0.997211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.9651324965132496, 0.9302301255230127, 0.857...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>0.986053</td>\n",
       "      <td>0.951185</td>\n",
       "      <td>0.934449</td>\n",
       "      <td>0.931660</td>\n",
       "      <td>0.963040</td>\n",
       "      <td>0.958856</td>\n",
       "      <td>0.918410</td>\n",
       "      <td>0.926778</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.935146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.898187</td>\n",
       "      <td>0.911402</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.937238</td>\n",
       "      <td>0.930962</td>\n",
       "      <td>0.997211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965132</td>\n",
       "      <td>[0.9302301255230127, 0.8570432357043235, 0.895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>0.951185</td>\n",
       "      <td>0.934449</td>\n",
       "      <td>0.931660</td>\n",
       "      <td>0.963040</td>\n",
       "      <td>0.958856</td>\n",
       "      <td>0.918410</td>\n",
       "      <td>0.926778</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.935146</td>\n",
       "      <td>0.963738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898187</td>\n",
       "      <td>0.911402</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.937238</td>\n",
       "      <td>0.930962</td>\n",
       "      <td>0.997211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965132</td>\n",
       "      <td>0.930230</td>\n",
       "      <td>[0.8570432357043235, 0.8953974895397491, 0.894...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3630</th>\n",
       "      <td>0.934449</td>\n",
       "      <td>0.931660</td>\n",
       "      <td>0.963040</td>\n",
       "      <td>0.958856</td>\n",
       "      <td>0.918410</td>\n",
       "      <td>0.926778</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.935146</td>\n",
       "      <td>0.963738</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911402</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.937238</td>\n",
       "      <td>0.930962</td>\n",
       "      <td>0.997211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965132</td>\n",
       "      <td>0.930230</td>\n",
       "      <td>0.857043</td>\n",
       "      <td>[0.8953974895397491, 0.894002789400279]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3631 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.011855  0.027197  0.036960  0.032775  0.037308  0.031032  0.036262   \n",
       "1     0.027197  0.036960  0.032775  0.037308  0.031032  0.036262  0.032078   \n",
       "2     0.036960  0.032775  0.037308  0.031032  0.036262  0.032078  0.025453   \n",
       "3     0.032775  0.037308  0.031032  0.036262  0.032078  0.025453  0.038703   \n",
       "4     0.037308  0.031032  0.036262  0.032078  0.025453  0.038703  0.028940   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3626  0.981869  0.975593  0.986053  0.951185  0.934449  0.931660  0.963040   \n",
       "3627  0.975593  0.986053  0.951185  0.934449  0.931660  0.963040  0.958856   \n",
       "3628  0.986053  0.951185  0.934449  0.931660  0.963040  0.958856  0.918410   \n",
       "3629  0.951185  0.934449  0.931660  0.963040  0.958856  0.918410  0.926778   \n",
       "3630  0.934449  0.931660  0.963040  0.958856  0.918410  0.926778  0.923291   \n",
       "\n",
       "            7         8         9   ...        21        22        23  \\\n",
       "0     0.032078  0.025453  0.038703  ...  0.029289  0.030683  0.030683   \n",
       "1     0.025453  0.038703  0.028940  ...  0.030683  0.030683  0.030683   \n",
       "2     0.038703  0.028940  0.020223  ...  0.030683  0.030683  0.037657   \n",
       "3     0.028940  0.020223  0.024059  ...  0.030683  0.037657  0.034170   \n",
       "4     0.020223  0.024059  0.016736  ...  0.037657  0.034170  0.034868   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3626  0.958856  0.918410  0.926778  ...  0.917713  0.910042  0.895397   \n",
       "3627  0.918410  0.926778  0.923291  ...  0.910042  0.895397  0.898187   \n",
       "3628  0.926778  0.923291  0.935146  ...  0.895397  0.898187  0.911402   \n",
       "3629  0.923291  0.935146  0.963738  ...  0.898187  0.911402  0.923291   \n",
       "3630  0.935146  0.963738  0.946304  ...  0.911402  0.923291  0.937238   \n",
       "\n",
       "            24        25        26        27        28        29  \\\n",
       "0     0.030683  0.037657  0.034170  0.034868  0.049477  0.056485   \n",
       "1     0.037657  0.034170  0.034868  0.049477  0.056485  0.052266   \n",
       "2     0.034170  0.034868  0.049477  0.056485  0.052266  0.055091   \n",
       "3     0.034868  0.049477  0.056485  0.052266  0.055091  0.048117   \n",
       "4     0.049477  0.056485  0.052266  0.055091  0.048117  0.053696   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "3626  0.898187  0.911402  0.923291  0.937238  0.930962  0.997211   \n",
       "3627  0.911402  0.923291  0.937238  0.930962  0.997211  1.000000   \n",
       "3628  0.923291  0.937238  0.930962  0.997211  1.000000  0.965132   \n",
       "3629  0.937238  0.930962  0.997211  1.000000  0.965132  0.930230   \n",
       "3630  0.930962  0.997211  1.000000  0.965132  0.930230  0.857043   \n",
       "\n",
       "                                                     0   \n",
       "0     [0.052266387726638774, 0.05509065550906553, 0....  \n",
       "1     [0.05509065550906553, 0.04811715481171547, 0.0...  \n",
       "2     [0.04811715481171547, 0.05369595536959554, 0.0...  \n",
       "3     [0.05369595536959554, 0.042887029288702916, 0....  \n",
       "4     [0.042887029288702916, 0.04497907949790794, 0....  \n",
       "...                                                 ...  \n",
       "3626      [1.0, 0.9651324965132496, 0.9302301255230127]  \n",
       "3627  [0.9651324965132496, 0.9302301255230127, 0.857...  \n",
       "3628  [0.9302301255230127, 0.8570432357043235, 0.895...  \n",
       "3629  [0.8570432357043235, 0.8953974895397491, 0.894...  \n",
       "3630            [0.8953974895397491, 0.894002789400279]  \n",
       "\n",
       "[3631 rows x 31 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, sc = ts_train_test_normalize(data,30,3)\n",
    "\n",
    "X_train_see= pd.DataFrame(np.reshape(X_train, (X_train.shape[0], X_train.shape[1])))\n",
    "Y_train_see= pd.DataFrame(Y_train)\n",
    "pd.concat([X_train_see, Y_train_see], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_pred_plot(preds):\n",
    "    \n",
    "    actual_pred = pd.DataFrame(columns = ['Close','prediction'])\n",
    "    actual_pred['Close']= data.loc['2002-01-01':'2016-10-14'][0:len(preds)]\n",
    "    actual_pred['prediction'] = preds[:,0]\n",
    "    \n",
    "    from keras.metrics import MeanSquaredError\n",
    "    m=MeanSquaredError()\n",
    "    m.update_state(np.array(actual_pred['Close']), np.array(actual_pred['prediction']))\n",
    "    return (m.result().numpy(), actual_pred.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train, y_train, X_test, sc):\n",
    "    # create a model\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, LSTM, Dropout,SimpleRNN\n",
    "    from keras.optimizers import SGD\n",
    "    \n",
    "    # The LSTM architecture\n",
    "    my_LSTM_model = Sequential()\n",
    "    my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(seq_length,5), activation='tanh'))\n",
    "    #my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "    #my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "    my_LSTM_model.add(LSTM(units=64, activation='tanh'))\n",
    "    #my_LSTM_model.add(Dropout(keep_prob))\n",
    "    my_LSTM_model.add(Dense(units=2))\n",
    "\n",
    "    # Compiling\n",
    "    my_LSTM_model.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
    "    # Fitting to the training set\n",
    "    my_LSTM_model.fit(X_train,y_train,epochs=50,batch_size=150, verbose=0)\n",
    "\n",
    "    LSTM_prediction = my_LSTM_model.predict(X_test)\n",
    "    LSTM_prediction = sc.inverse_transform(LSTM_prediction)\n",
    "\n",
    "    return my_LSTM_model, LSTM_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습용/테스트용 데이터 생성\n",
    "# train_size = int(len(dataY) * 0.7)\n",
    "# test_size = len(dataY) - train_size\n",
    "\n",
    "\n",
    "# # 데이터 생성\n",
    "# trainX = np.array(dataX[0:train_size])\n",
    "# trainY = np.array(dataY[0:train_size])\n",
    "\n",
    "# # 테스트용 데이터 생성\n",
    "# testX = np.array(dataX[train_size:len(dataX)])\n",
    "# testY = np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "# print('All data size: ',len(dataY))\n",
    "# print('Training data size: ',train_size)\n",
    "# print('Testing data size: ',test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_LSTM_model, LSTM_prediction = LSTM_model(trainX, trainY, testX, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_6_input to have shape (50, 5) but got array with shape (30, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\YooByeongJu\\Finking\\Prediction\\tensor_pre.ipynb 셀 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/YooByeongJu/Finking/Prediction/tensor_pre.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m my_LSTM_model, LSTM_prediction \u001b[39m=\u001b[39m LSTM_model(X_train, Y_train, X_test, sc)\n",
      "\u001b[1;32mc:\\Users\\YooByeongJu\\Finking\\Prediction\\tensor_pre.ipynb 셀 22\u001b[0m in \u001b[0;36mLSTM_model\u001b[1;34m(X_train, y_train, X_test, sc)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YooByeongJu/Finking/Prediction/tensor_pre.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m my_LSTM_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mSGD(lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, decay\u001b[39m=\u001b[39m\u001b[39m1e-7\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, nesterov\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YooByeongJu/Finking/Prediction/tensor_pre.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Fitting to the training set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/YooByeongJu/Finking/Prediction/tensor_pre.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m my_LSTM_model\u001b[39m.\u001b[39;49mfit(X_train,y_train,epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YooByeongJu/Finking/Prediction/tensor_pre.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m LSTM_prediction \u001b[39m=\u001b[39m my_LSTM_model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YooByeongJu/Finking/Prediction/tensor_pre.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m LSTM_prediction \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39minverse_transform(LSTM_prediction)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:854\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    853\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 854\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    855\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    856\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    857\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    858\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    859\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m    860\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    861\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    862\u001b[0m     validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[0;32m    863\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m    864\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m    865\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m    866\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    867\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m    868\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m    869\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m    870\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[0;32m    871\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m    872\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m    873\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m    874\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:698\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[0;32m    675\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    676\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    693\u001b[0m ):\n\u001b[0;32m    694\u001b[0m     batch_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_validate_or_infer_batch_size(\n\u001b[0;32m    695\u001b[0m         batch_size, steps_per_epoch, x\n\u001b[0;32m    696\u001b[0m     )\n\u001b[1;32m--> 698\u001b[0m     x, y, sample_weights \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49m_standardize_user_data(\n\u001b[0;32m    699\u001b[0m         x,\n\u001b[0;32m    700\u001b[0m         y,\n\u001b[0;32m    701\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    702\u001b[0m         class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m    703\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    704\u001b[0m         check_steps\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         steps_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps_per_epoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    706\u001b[0m         steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m    707\u001b[0m         validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[0;32m    708\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m    709\u001b[0m     )\n\u001b[0;32m    711\u001b[0m     \u001b[39mif\u001b[39;00m validation_data:\n\u001b[0;32m    712\u001b[0m         val_x, val_y, val_sample_weights \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_prepare_validation_data(\n\u001b[0;32m    713\u001b[0m             validation_data, batch_size, validation_steps\n\u001b[0;32m    714\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2650\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2641\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2642\u001b[0m     \u001b[39mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2643\u001b[0m     \u001b[39mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2646\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(_is_symbolic_tensor(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m all_inputs)\n\u001b[0;32m   2647\u001b[0m ):\n\u001b[0;32m   2648\u001b[0m     \u001b[39mreturn\u001b[39;00m [], [], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2650\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_tensors(\n\u001b[0;32m   2651\u001b[0m     x,\n\u001b[0;32m   2652\u001b[0m     y,\n\u001b[0;32m   2653\u001b[0m     sample_weight,\n\u001b[0;32m   2654\u001b[0m     run_eagerly\u001b[39m=\u001b[39;49mrun_eagerly,\n\u001b[0;32m   2655\u001b[0m     dict_inputs\u001b[39m=\u001b[39;49mdict_inputs,\n\u001b[0;32m   2656\u001b[0m     is_dataset\u001b[39m=\u001b[39;49mis_dataset,\n\u001b[0;32m   2657\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2658\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2659\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2691\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2688\u001b[0m \u001b[39m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2689\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset)):\n\u001b[0;32m   2690\u001b[0m     \u001b[39m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2691\u001b[0m     x \u001b[39m=\u001b[39m training_utils_v1\u001b[39m.\u001b[39;49mstandardize_input_data(\n\u001b[0;32m   2692\u001b[0m         x,\n\u001b[0;32m   2693\u001b[0m         feed_input_names,\n\u001b[0;32m   2694\u001b[0m         feed_input_shapes,\n\u001b[0;32m   2695\u001b[0m         check_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2696\u001b[0m         exception_prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2697\u001b[0m     )\n\u001b[0;32m   2699\u001b[0m \u001b[39m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2700\u001b[0m \u001b[39m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2701\u001b[0m \u001b[39m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[39m# standardization code.\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:731\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[39mfor\u001b[39;00m dim, ref_dim \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data_shape, shape):\n\u001b[0;32m    726\u001b[0m                 \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    727\u001b[0m                     ref_dim \u001b[39m!=\u001b[39m dim\n\u001b[0;32m    728\u001b[0m                     \u001b[39mand\u001b[39;00m ref_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    729\u001b[0m                     \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    730\u001b[0m                 ):\n\u001b[1;32m--> 731\u001b[0m                     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    732\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mError when checking \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m                         \u001b[39m+\u001b[39m exception_prefix\n\u001b[0;32m    734\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    735\u001b[0m                         \u001b[39m+\u001b[39m names[i]\n\u001b[0;32m    736\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to have shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(shape)\n\u001b[0;32m    738\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m but got array with shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(data_shape)\n\u001b[0;32m    740\u001b[0m                     )\n\u001b[0;32m    741\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_6_input to have shape (50, 5) but got array with shape (30, 1)"
     ]
    }
   ],
   "source": [
    "my_LSTM_model, LSTM_prediction = LSTM_model(X_train, Y_train, X_test, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data size:  5137\n",
      "Training data size:  3595\n",
      "Testing data size:  1542\n"
     ]
    }
   ],
   "source": [
    "# 학습용/테스트용 데이터 생성\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "\n",
    "\n",
    "# 데이터 생성\n",
    "trainX = np.array(dataX[0:train_size])\n",
    "trainY = np.array(dataY[0:train_size])\n",
    "\n",
    "# 테스트용 데이터 생성\n",
    "testX = np.array(dataX[train_size:len(dataX)])\n",
    "testY = np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "print('All data size: ',len(dataY))\n",
    "print('Training data size: ',train_size)\n",
    "print('Testing data size: ',test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  Tensor(\"Placeholder:0\", shape=(None, 30, 5), dtype=float32)\n",
      "Y:  Tensor(\"Placeholder_1:0\", shape=(None, 1), dtype=float32)\n",
      "targets:  Tensor(\"Placeholder_2:0\", shape=(None, 1), dtype=float32)\n",
      "predictions:  Tensor(\"Placeholder_3:0\", shape=(None, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로우 플레이스홀더 생성\n",
    "# 입력 X, 출력 Y를 생성한다\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, [None, seq_length, input_data_column_cnt])\n",
    "print(\"X: \", X)\n",
    "Y = tf.compat.v1.placeholder(tf.float32, [None, 1])\n",
    "print(\"Y: \", Y)\n",
    "\n",
    "# 검증용 측정지표를 산출하기 위한 targets, predictions를 생성한다\n",
    "targets = tf.compat.v1.placeholder(tf.float32, [None, 1])\n",
    "print(\"targets: \", targets)\n",
    "\n",
    "predictions = tf.compat.v1.placeholder(tf.float32, [None, 1])\n",
    "print(\"predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input((seq_length, input_dim))\n",
    "lstm = LSTM(128, return_sequences=True, activation='tanh')(x)\n",
    "td = TimeDistributed(Dense(out_size, activation='softmax'))(lstm)\n",
    "second_input = Input((seq_len, out_size)) # object instanciated and hold as a var.\n",
    "out = merge([td, second_input], mode='mul')\n",
    "model = Model(input=[x, second_input], output=out) # second input provided to model.compile(...)\n",
    "\n",
    "# then I add two inputs\n",
    "model.fit([trainX, filter], trainY, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
